# Optical-Character-Recognition
This is a machine learning program I wrote in Python to do optical character recognition. The program reads the MNIST data which consists of data set containing images of handwritten digits. <br /><br />
The program takes the training data and fits a multivariate normal distribution to it using the fitNormal function, which finds the mean and covariance matrices. Then it uses these matrices to find the variance matrix. The formula used to find the means is: <img src="http://latex.codecogs.com/gif.latex?\hat{\mu}_j&space;=&space;\sum_{i=1}^{m}&space;\frac{x_{ij}}{m}" title="\hat{\mu}_j = \sum_{i=1}^{m} \frac{x_{ij}}{m}" />, where <img src="http://latex.codecogs.com/gif.latex?x_{ij}" title="x_{ij}" /> is the <img src="http://latex.codecogs.com/gif.latex?j_{th}" title="j_{th}" /> component of <img src="http://latex.codecogs.com/gif.latex?x_i" title="x_i" /> for a given data sample <img src="http://latex.codecogs.com/gif.latex?x_1,&space;x_2,&space;...,&space;x_m" title="x_1, x_2, ..., x_m" />. The covariance matrix is calculated using: <img src="http://latex.codecogs.com/gif.latex?\sum&space;=&space;\frac{X_c^T&space;X_c}{m}" title="\sum = \frac{X_c^T X_c}{m}" /> where <img src="http://latex.codecogs.com/gif.latex?X_c" title="X_c" /> is a m x n matrix whose <img src="http://latex.codecogs.com/gif.latex?i^{th}" title="i^{th}" /> row is the vector <img src="http://latex.codecogs.com/gif.latex?x_i&space;-&space;\hat{\mu}" title="x_i - \hat{\mu}" /> for the given data <img src="http://latex.codecogs.com/gif.latex?x_1,&space;x_2,&space;...,&space;x_m" title="x_1, x_2, ..., x_m" />. <br /> <br />
After the simple and complex model is evaluated in the combine function to obtain the combined model. This is done through the following formula: <img src="http://latex.codecogs.com/gif.latex?\overline{\sum}_d&space;=&space;\beta\sum_d&space;&plus;&space;(1-\beta)I\sigma^2" title="\overline{\sum}_d = \beta\sum_d + (1-\beta)I\sigma^2" /> where <img src="http://latex.codecogs.com/gif.latex?0&space;\leq\beta\leq1" title="0 \leq\beta\leq1" />, <img src="http://latex.codecogs.com/gif.latex?d" title="d" /> represents the digit and <img src="http://latex.codecogs.com/gif.latex?1-\beta" title="1-\beta" /> represents the weight of the simple model. <br /> <br />
Once the combined model is obtained, the prediction is carried on the testing data which calculates the log probabilities using the following formula: <img src="http://latex.codecogs.com/gif.latex?log(p)=-\frac{1}{2}(x-\mu)^T\sum\nolimits^{-1}(x-\mu)-\frac{n}{2}log(2\pi)-\frac{1}{2}log(|\sum|)" title="log(p)=-\frac{1}{2}(x-\mu)^T\sum\nolimits^{-1}(x-\mu)-\frac{n}{2}log(2\pi)-\frac{1}{2}log(|\sum|)" /> where <img src="http://latex.codecogs.com/gif.latex?\mu" title="\mu" /> is the mean matrix and <img src="http://latex.codecogs.com/gif.latex?\sum" title="\sum" /> is the covariance matrix.<br /><br />
After the log probabilities are found, the soft prediction is converted into hard prediction by the evaluate function and the correct and incorrect predictions are displayed.
